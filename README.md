
# DocBrief - Document Summarizer and Smart Assistant

**DocBrief** is a web-based application that allows users to upload PDF documents, generate concise summaries, and ask questions to a smart assistant that retrieves relevant information directly from the document. Using powerful AI and vector search technologies, **DocBrief** processes documents, splits them into manageable chunks, and retrieves information that answers specific user queries.

<p align="center">
  <img src="./doc_brief.gif" alt="DocBrief Demo">
</p>

## Features
- **PDF Upload**: Easily upload PDF documents for processing.
- **Smart Assistant**: Ask specific questions about the document. The assistant retrieves answers directly from the document content.
- **Document Splitting**: Large documents are automatically split into smaller chunks for efficient handling.
- **Embeddings and Vector Search**: Documents are converted into vector embeddings for fast and accurate information retrieval using powerful vector databases.
- **Custom Summarization**: Summaries are generated by querying the vector store. The app can integrate with various LLMs such as Google’s Generative AI or OpenAI's GPT models.
- **Monitoring**: Built-in monitoring using **LangSmith** to track app performance and execution.

## Prerequisites
- Python 3.7 or higher
- pip or pipenv for managing dependencies

## Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-username/DocBrief.git
   cd DocBrief
   ```

2. **Install dependencies**:
   Using `pipenv` (recommended):
   ```bash
   pipenv install
   pipenv shell
   ```
   Or with `pip` and `virtualenv`:
   ```bash
   python -m venv venv
   source venv/bin/activate  # For macOS/Linux
   venv\Scripts\activate     # For Windows
   pip install -r requirements.txt
   ```

3. **Update the `.env` file**:
   Create a `.env` file in the root directory of the project (if it doesn't exist already) and add your API keys and configuration:
   ```env
   GOOGLE_API_KEY=your-google-api-key
   ```

4. **Run the app**:
   ```bash
   python app.py
   ```
   The app will be available at `http://127.0.0.1:5000`.

## How It Works

Once you upload a PDF document, the app processes it by splitting it into smaller chunks and generating vector embeddings. The embeddings are stored in a vector store, and queries are used to retrieve relevant information. You can ask the assistant specific questions about the document content, and it will dynamically search the document and provide accurate answers.

### Example Queries:
The smart assistant can answer a wide range of questions depending on the content of the uploaded document. Here are some example queries that the assistant can handle:

#### For a CV (Curriculum Vitae):
1. **"What is the candidate's experience in software development?"**
2. **"Can you list the candidate's skills?"**
3. **"Where has the candidate worked previously?"**

#### For a Research Paper:
1. **"What is the main research question of this paper?"**
2. **"What methodology was used in this study?"**
3. **"Summarize the conclusion of the paper."**

#### For General Documents:
1. **"Give me a summary of the document in 3 sentences."**
2. **"What are the key findings mentioned in this report?"**
3. **"Tell me about the main topic of the document."**

### Vector Databases and FAISS

Vector databases like **FAISS** (Facebook AI Similarity Search) allow us to efficiently store and retrieve high-dimensional vector representations of data. These vector representations (or embeddings) are generated by AI models, and FAISS enables fast similarity searches, making it possible to retrieve relevant chunks of a document based on user queries.

You can also use **Pinecone**, another popular vector database, which simplifies deploying vector search in the cloud. Pinecone supports scaling vector search with high performance and is a great alternative to FAISS if you're looking for a cloud-based solution.

## Customizing the LLM

While **DocBrief** uses Google’s Generative AI by default for summarization, you can modify the app to use other LLMs such as OpenAI's GPT models or any custom LLM by updating the summarization and query-handling logic.

## Monitoring with LangSmith

The app integrates with **LangSmith** for tracking and analyzing application performance. LangSmith helps monitor the processing of documents, summarization, and the execution of LangChain-based workflows.

## Technologies Used
- **Flask**: Web framework for building the application.
- **LangChain**: A framework to work with language models and document processing tools.
- **FAISS**: A vector store for fast and efficient similarity search.
- **Pinecone**: A cloud-based vector database for scalable search.
- **Google Generative AI**: For generating document embeddings and summaries.
- **LangSmith**: For monitoring and tracking LangChain-based applications.

## Contributing

Contributions are welcome! Feel free to fork the repository, open issues, and submit pull requests.
